# Набор конфигураций для определяющий поведение агентов
behaviors:
  RollerBall:
    # Определения алгоритма обучения. В данном случае используется Proximal Policy Optimization - это алгоритм обучения с подкреплением
    trainer_type: ppo
    # Гиперпараметры модели
    hyperparameters:
      # Количество опыта на каждой итерации градиентного спуска.
      batch_size: 10
      #  Количество опыта, необходимое для начала обновления модели политики
      buffer_size: 100
      # Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска.
      learning_rate: 3.0e-4
      # Сила регуляризации энтропии - необходимо для рандомизации политики. Необходимо для исследования пространства действий во время обучения. 
      beta: 5.0e-4
      # Параметр допустимого расхождения между старой и новой политикой. Влияет на скорость обучения и стабильность обновлений политики.
      epsilon: 0.2
      #  Процент использования текущей политики для предсказаний
      lambd: 0.99
      # Количество проходов, которые необходимо выполнить через буфер опыта при выполнении оптимизации градиентного спуска. Количество эпох перед оптимизацией градиентного спуска
      num_epoch: 3
      # Определяет изменение скорости обучение во времени.
      learning_rate_schedule: linear
    # Настройки нейронной сети
    network_settings:
      # Применяется ли нормализация к входным данным векторного наблюдения. 
      normalize: false
      # Количество нейронов в скрытых слоях нейронной сети
      hidden_units: 128
      # Количество скрытых слоев в нейронной сети после приема входящих данных.
      num_layers: 2
    # Параметры внешних и внутренних сигналов вознаграждения
    reward_signals:
      extrinsic:
        # Коэффициент дисконтирования будущих вознаграждений, поступающих от окружающей среды
        gamma: 0.99
        # Мультипликатор вознаграждения окружающей среды
        strength: 1.0
    # Количество шагов необходимого для завершения обучения
    max_steps: 500000
    # Количество шагов необходимого для добавления опыта в буфер.
    time_horizon: 64
    # Количество опыта необходимое для отображения статистики обучения
    summary_freq: 10000
    # Использование многопоточности
    threaded: true